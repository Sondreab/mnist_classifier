# -*- coding: utf-8 -*-
"""Assignment_2_extra_layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ADpszug777jx6OxCEwAXryd4qHeNszzi
"""

import numpy as np
from urllib import request
import matplotlib.pyplot as plt
import tqdm
import gzip
import pickle
import os

filename = [
["training_images","train-images-idx3-ubyte.gz"],
["test_images","t10k-images-idx3-ubyte.gz"],
["training_labels","train-labels-idx1-ubyte.gz"],
["test_labels","t10k-labels-idx1-ubyte.gz"]
]
SAVE_PATH = "data"

def download_mnist():
    os.makedirs(SAVE_PATH, exist_ok=True)
    base_url = "http://yann.lecun.com/exdb/mnist/"
    for name in filename:
        print("Downloading "+name[1]+"...")
        save_path = os.path.join(SAVE_PATH, name[1])
        request.urlretrieve(base_url+name[1], save_path)
    print("Download complete.")

def save_mnist():
    mnist = {}
    for name in filename[:2]:
        path = os.path.join(SAVE_PATH, name[1])
        with gzip.open(path, 'rb') as f:
            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)
    for name in filename[-2:]:
        path = os.path.join(SAVE_PATH, name[1])
        with gzip.open(path, 'rb') as f:
            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)
    save_path = os.path.join(SAVE_PATH, "mnist.pkl")
    with open(save_path, 'wb') as f:
        pickle.dump(mnist,f)
    print("Save complete.")

def init():
    download_mnist()
    save_mnist()

def load():
    save_path = os.path.join(SAVE_PATH, "mnist.pkl")
    with open(save_path,'rb') as f:
        mnist = pickle.load(f)
    return mnist["training_images"], mnist["training_labels"], mnist["test_images"], mnist["test_labels"]

def should_early_stop(validation_loss, num_steps=3):
    """
    Returns true if the validation loss increases
    or stays the same for num_steps.
    --
    validation_loss: List of floats
    num_steps: integer
    """
    if len(validation_loss) < num_steps+1:
        return False

    is_increasing = [validation_loss[i] <= validation_loss[i+1] for i in range(-num_steps-1, -1)]
    return sum(is_increasing) == len(is_increasing)

def train_val_split(X, Y, val_percentage):
  """
    Selects samples from the dataset randomly to be in the validation set. Also, shuffles the train set.
    --
    X: [N, num_features] numpy vector,
    Y: [N, K] numpy vector
    val_percentage: amount of data to put in validation set
  """
  dataset_size = X.shape[0]
  idx = np.arange(0, dataset_size)
  np.random.shuffle(idx) 
  
  train_size = int(dataset_size*(1-val_percentage))
  idx_train = idx[:train_size]
  idx_val = idx[train_size:]
  X_train, Y_train = X[idx_train], Y[idx_train]
  X_val, Y_val = X[idx_val], Y[idx_val]
  return X_train, Y_train, X_val, Y_val

def shuffle_training_set(X,Y):
  dataset_size = X.shape[0]
  idx = np.arange(0, dataset_size)
  np.random.shuffle(idx)
  
  X, Y = X[idx], Y[idx]
  
  return X, Y

def onehot_encode(Y, n_classes=10):
    onehot = np.zeros((Y.shape[0], n_classes))
    onehot[np.arange(0, Y.shape[0]), Y] = 1
    return onehot

def bias_trick(X):
    """
    X: shape[batch_size, num_features(784)]
    -- 
    Returns [batch_size, num_features+1 ]
    """
    return np.concatenate((X, np.ones((len(X), 1))), axis=1)

def normalize_pixels(X):
    return (X / 127.5) - np.ones(np.shape(X))

def weight_initialization(input_units, output_units):
    weight_shape = (output_units, input_units)
    return np.random.uniform(-1, 1, weight_shape)

def weight_initialization_normal_dist(input_units, output_units):
    weight_shape = (output_units, input_units)
    return np.random.normal(0, 1/np.sqrt(input_units), weight_shape)

def softmax(z):
    """
    Applies the softmax activation function for the vector a.
    --
    z: shape: [batch_size, num_classes]. Activation of the output layer before activation
    --
    Returns: [batch_size, num_classes] numpy vector
    """
    z_exp = np.exp(z)
    return z_exp / z_exp.sum(axis=1, keepdims=True) #axis = 1 has to be changed to axis = 0 for N=1

def sigmoid(z):
    """
    Applies the sigmoid activation function for the vector z.
    --
    z: shape: [batch_size, num_hidden_layer]. Activation of the hidden layer before activation
    --
    Returns: [batch_size, num_hidden_layer] numpy vector
    """
    if improved_sigmoid:
      return 1.7159*np.tanh((2.0/3.0)*z)
    else:
      return 1 / (1 + np.exp(-z))

def forward_ji(X, w):
    """
    Performs a forward pass to the hidden layer
    Used both from input->hidden and hidden->hidden
    --
    X: shape: [batch_size, num_features(784+1)] numpy vector. Input batch of images
    w: shape: [num_hidden_layer(J), num_features] numpy vector. Weight from input->hidden layer
        OR    [num_hidden_layer(J), num_hidden_layer(J)] numpy vector. Weight from hidden->hidden layer
    --
    Returns: [batch_size, num_hidden_layer] numpy vector
    """
    zj = X.dot(w.T)
    return sigmoid(zj)

def forward_kj(X, w):
    zk = X.dot(w.T)
    return softmax(zk)

def forward(X, w1, wj, w2):
    """
    Performs a forward pass through the network
    --
    X: shape: [batch_size, num_features(784+1)] numpy vector. Input batch of images
    w1: shape: [num_hidden_layer(J), num_features(I=784+1)] numpy vector. Weight from input->hidden layer
    wj: shape: [num_hidden_layer(J), num_hidden_layer(J)] numpy vector. Weight from hidden->hidden layer
    w2: shape: [num_classes(K=10), num_hidden_layer(J)] numpy vector. Weight from hidden layer->output
    --
    Returns: [batch_size, num_classes] numpy vector
    """
    a1 = forward_ji(X, w1)
    a2 = forward_ji(a1,wj)
    y = forward_kj(a2, w2)
    
    #Checking if values in y <= 1e-8, if they are: set them to 1e-8
    #y[y<=1e-8] = 1e-8
    return y

def calculate_accuracy(X, targets, w1, wjj, w2):
    """
    Calculated the accuracy of the network.
    ---
    X: shape: [batch_size, num_features(784+1)] numpy vector. Input batch of images
    targets: shape: [batch_size, num_classes] numpy vector. Targets/label of images
    w1: shape: [num_hidden_layer(J), num_features(I=784+1)] numpy vector. Weight from input->hidden layer
    wj: shape: [num_hidden_layer(J), num_hidden_layer(J)] numpy vector. Weight from hidden->hidden layer
    w2: shape: [num_classes(K=10), num_hidden_layer(J)] numpy vector. Weight from hidden layer->output
    --
    Returns float
    """
    output = forward(X, w1, wjj, w2)
    predictions = output.argmax(axis=1)
    targets = targets.argmax(axis=1)
    return (predictions == targets).mean()

def cross_entropy_loss(X, targets, w1, wjj, w2):
    """
    Computes the cross entropy loss given the input vector X and the target vector.
    ---
    X: shape: [batch_size, num_features(784+1)] numpy vector. Input batch of images
    targets: shape: [batch_size, num_classes] numpy vector. Targets/label of images
    w1: shape: [num_hidden_layer(J), num_features(I=784+1)] numpy vector. Weight from input->hidden layer
    wj: shape: [num_hidden_layer(J), num_hidden_layer(J)] numpy vector. Weight from hidden->hidden layer
    w2: shape: [num_classes(K=10), num_hidden_layer(J)] numpy vector. Weight from hidden layer->output
    --
    Returns float
    """
    output = forward(X, w1, wjj, w2)
    assert output.shape == targets.shape
    #protection to ensure that output[i] is not negative or too small
    output[output <= 1e-8] = 1e-8
    log_y = np.log(output)
    cross_entropy = -targets * log_y
    #print(cross_entropy.shape)
    return cross_entropy.mean()

def hidden_activation_derivative(a):
  if improved_sigmoid:
      df = (2.0/3.0)*(1.7159 - (1.0/1.7159)*a**2)
  else:
      df = a*(1 - a) # shape = (N,J)
  return df

def gradient_descent(X, targets, w1, wj, w2, v1, vj, v2):
    """
    Performs gradient descents for all weights in the network.
    ---
    X: shape: [batch_size, num_features(784+1)] numpy vector. Input batch of images
    targets: shape: [batch_size, num_classes] numpy vector. Targets/label of images
    w1: shape: [num_hidden_layer(J), num_features(I=784+1)] numpy vector. Weight from input->hidden layer
    wj: shape: [num_hidden_layer(J), num_hidden_layer(J)] numpy vector. Weight from hidden->hidden layer
    w2: shape: [num_classes(K=10), num_hidden_layer(J)] numpy vector. Weight from hidden layer->output
    --
    Returns updated weights, with same shapes
    """

    # Since we are taking the .mean() of our loss, we get the normalization factor to be 1/(N*K)
    # The normalization factor is identical for all weights in the network (For multi-layer neural-networks as well.)
    
    normalization_factor = X.shape[0] * targets.shape[1] # batch_size * num_classes
    outputs = forward(X, w1, wj, w2) # shape = (num_samples, num_classes) 
    delta_k = - (targets - outputs) # (num_samples = N, num_classes = K)
    
    # Calculating new w_{kj} 
    a1 = forward_ji(X, w1) # shape = (N, J) 
    a2 = forward_ji(a1, wj) # shape = (N,J)
    
    dw2 = delta_k.T.dot(a2) # shape = (K, J)
    dw2 = dw2 / normalization_factor # Normalize gradient equally as loss normalization
    assert dw2.shape == w2.shape, "dw shape was: {}. Expected: {}".format(dw.shape, w2.shape)
        
    # Backprop w_{jj} to calculate new w_{ji}
    
    df2 = hidden_activation_derivative(a2)
    
    delta_j2 = df2 * delta_k.dot(w2) # (N, J) * (N, K) x (K , J) = (N, J) 
    dwj = delta_j2.T.dot(a1) # (J, N) x (N, J)
    dwj = dwj / normalization_factor
    
    # Backprop w_{kj} to calculate new w_{jj}
   
    df1 = hidden_activation_derivative(a1)
    
    delta_j1 = df1 * delta_j2.dot(wj) # (N, J) * (N, J) x (J , J) = (N, J) 
    dw1 = delta_j1.T.dot(X) # (J, N) x (N, I)
    dw1 = dw1 / normalization_factor
  
       
    # update
    if use_momentum:
      v1 = momentum_mu * v1 - learning_rate * dw1
      vj = momentum_mu * vj - learning_rate * dwj
      v2 = momentum_mu * v2 - learning_rate * dw2
      w1 = w1 + v1
      wj = wj + vj
      w2 = w2 + v2
    else:
      w1 = w1 - learning_rate * dw1 # shape = (J, I) 
      wj = wj - learning_rate * dwj # shape = (J, J) 
      w2 = w2 - learning_rate * dw2 # shape = (K, J)
    
    return w1, wj, w2, v1, vj, v2

def train_loop(w1, wj, w2, v1, vj, v2):
    training_it = 0
    for e in range(max_epochs): # Epochs
        #shuffle the training set after each epoch (3.a)
        if should_shuffle:
          X_epoch, Y_epoch = shuffle_training_set(X_train,Y_train)
        else:
          X_epoch = X_train
          Y_epoch = Y_train
        for i in tqdm.trange(num_batches):
            training_it += 1
            #print("\n train loop No: {}".format(i))
            X_batch = X_epoch[i*batch_size:(i+1)*batch_size]
            Y_batch = Y_epoch[i*batch_size:(i+1)*batch_size]

            w1, wj, w2, v1, vj, v2 = gradient_descent(X_batch, Y_batch, w1, wj, w2, v1, vj, v2)
            #print(cross_entropy_loss(X_batch, Y_batch, w))
            if i % check_step == 0:
                TRAINING_STEP.append(training_it)
                # Loss
                TRAIN_LOSS.append(cross_entropy_loss(X_epoch, Y_epoch, w1, wj, w2))
                TEST_LOSS.append(cross_entropy_loss(X_test, Y_test, w1, wj, w2))
                VAL_LOSS.append(cross_entropy_loss(X_val, Y_val, w1, wj, w2))
                

                TRAIN_ACC.append(calculate_accuracy(X_epoch, Y_epoch, w1, wj, w2))
                VAL_ACC.append(calculate_accuracy(X_val, Y_val, w1, wj, w2))
                TEST_ACC.append(calculate_accuracy(X_test, Y_test, w1, wj, w2))
                
                W = [w1, wj, w2]
                LAST_WEIGHTS.append(W)
                if len(LAST_WEIGHTS) > early_stop_num_steps-1:
                    W = LAST_WEIGHTS.pop(0)
                
                if should_early_stop(VAL_LOSS, early_stop_num_steps):
                    print(VAL_LOSS[-early_stop_num_steps+1:])
                    print("early stopping.")
                    return W[0], W[1], W[2]
            
                
    return w1, wj, w2

init()

X_train, Y_train, X_test, Y_test = load()

# Pre-process data
X_train, X_test = normalize_pixels(X_train), normalize_pixels(X_test)
X_train = bias_trick(X_train)
X_test = bias_trick(X_test)
Y_train, Y_test = onehot_encode(Y_train), onehot_encode(Y_test)

X_train, Y_train, X_val, Y_val = train_val_split(X_train, Y_train, 0.1)

# Hyperparameters
batch_size = 32
learning_rate = 0.1
momentum_mu = 0.9
early_stop_num_steps = 5
num_batches = X_train.shape[0] // batch_size
check_step = num_batches // 10
max_epochs = 15

# Toggles
#should_check_gradient = False ### NOT IMPLEMENTED FOR MORE THAN 1 HIDDEN LAYER
should_shuffle = True
improved_sigmoid = True
normal_dist_weights = True
use_momentum = True


N = X_train.shape[0]
I = X_train.shape[1]
J = 59
K = 10

### TO MINIMIZE CHANGE IN NAMES AND ADDING INDECIES ETC., BOTH HIDDEN LAYERS
### ARE REFERENCED WITH THE LETTER J, SO WE HAVE:
### w_1 = w_ji (input->hidden1)
### w_jj (hidden1->hidden2)
### w_2 = w_kj (hidden2->output)

# Initialize weights

if normal_dist_weights:
  w_1 = weight_initialization_normal_dist(I, J)
  w_jj = weight_initialization_normal_dist(J, J)
  w_2 = weight_initialization_normal_dist(J, K)
else:
  w_1 = weight_initialization(I, J)
  w_jj = weight_initialization(J, J)
  w_2 = weight_initialization(J, K)
  
# Initialize momentum
v_1 = np.zeros(w_1.shape)
v_jj = np.zeros(w_jj.shape)
v_2 = np.zeros(w_2.shape)


# Tracking variables
TRAINING_STEP = []
TRAIN_LOSS = []
TEST_LOSS = []
VAL_LOSS = []
TRAIN_ACC = []
TEST_ACC = []
VAL_ACC = []
LAST_WEIGHTS = []

w_1, w_jj, w_2 = train_loop(w_1, w_jj, w_2, v_1, v_jj, v_2)

plt.plot(TRAINING_STEP, TRAIN_LOSS, label="Training loss")
plt.plot(TRAINING_STEP, TEST_LOSS, label="Testing loss")
plt.plot(TRAINING_STEP, VAL_LOSS, label="Validation loss")
plt.legend()
plt.ylim([0, 0.25])
plt.show()

plt.clf()
plt.plot(TRAINING_STEP, TRAIN_ACC, label="Training accuracy")
plt.plot(TRAINING_STEP, TEST_ACC, label="Testing accuracy")
plt.plot(TRAINING_STEP, VAL_ACC, label="Validation accuracy")
plt.ylim([0.1, 1.0])
plt.legend()
plt.show()

plt.clf()
    

print("Val Loss 'best': {}".format(VAL_LOSS[-early_stop_num_steps+1]))
print("TEST Loss 'best': {}".format(TEST_LOSS[-early_stop_num_steps+1]))
print("VAL acc 'best': {}".format(VAL_ACC[-early_stop_num_steps+1]))

# Code for checking our trained net

index = np.random.randint(0,6000)
print(index)

number = Y_val[index]
print(number)
picture = X_val[index]
picture = picture[:-1]
picture = picture.reshape(28,28)
plt.imshow(picture, cmap="gray")
plt.show()

X = X_val[index]
#Because of how numpy arrays swap dimensions from (I,) to (N,I) when N>1, our code only works fro N>1
X_hack = np.zeros((2,785))
X_hack[0] = X
X_hack[1] = X

y = forward(X_hack, w_1, w_jj, w_2)
print(y[1])